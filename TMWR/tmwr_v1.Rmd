---
title: "Learning TidyModels"
author: 'Jeremiah Wang'
date: "November 2020"
output:
  html_document:
    theme: simplex
    highlight: pygments
    code_folding: hide
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
    toc_depth: 2
    df_print: paged
---
 
```{r}
#https://datatables.net/reference/option/
options(DT.options = list(scrollX = TRUE, pagin=TRUE, fixedHeader = TRUE, searchHighlight = TRUE))
```

```{r include=FALSE}
library(DataExplorer);library(data.table);library(dlookr);
library(extrafont);library(formattable);library(GGally);library(here);
library(janitor);library(lubridate);library(naniar);
library(PerformanceAnalytics);
library(plotly);library(RColorBrewer);library(readxl);
library(skimr);library(tidyverse);library(scales)

library(caret);library(tidymodels);library(h2o)

library(modeldata) #used for learning tidymodels
```

# Resource

* https://www.tmwr.org/ames.html

# BASICS
# Chapter 4: The Ames housing data
```{r}
data('ames');a = ames

a = a %>%
  clean_names() %>% 
  select(sort(tidyselect::peek_vars())) %>% 
  select(
    where(is.Date),
    where(is.character),
    where(is.factor),
    where(is.numeric)
    )

a %>% head %>% DT::datatable()
```

## 4.1 exploring important features

distribution of outcome var, sale price
```{r}
a %>% plot_ly(x = ~sale_price) %>% add_boxplot()
a %>% plot_ly(x = ~sale_price) %>% add_histogram()
```
<h style="color: blue; font-size:14px;">1. right skewed due to many high outliers; we should log transform</h>

```{r}
ggplotly(a %>% ggplot(aes(sale_price)) + geom_histogram(bins = 50) + scale_x_log10())
```

```{r}
a$sale_price = log10(a$sale_price)
```


<h style="color: blue; font-size:14px;">transforming the output var will probably result in better models than using the untransformed data.</h>

<h style="color: red; font-size:14px;">The downside to transforming the outcome is mostly related to interpretation.</h>

# Chapter 5: Spending(Splitting) our data

## 5.1 common methods for splitting data
```{r}
set.seed(123)

(split = a %>% initial_split(prob = 0.8, strata = sale_price))

train = training(split)
test = testing(split)

```

# Chapter 6: Feature engineering with recipes

## 6.1 a simple recipe for the ames housing data

<h style="color: blue; font-size:14px;"> Unlike the formula method inside a modeling function, the recipe defines the steps without immediately executing them; it is only a specification of what should be done. </h>

this is equivalent to ...
```{r}
lm(sale_price ~ neighborhood + log10(gr_liv_area) + year_built + bldg_type)
```
...this
```{r}
simple_ames_recipe <- 
  recipe(sale_price ~ neighborhood + gr_liv_area + year_built + bldg_type,
         data = train) %>%
  step_log(gr_liv_area, base = 10) %>% 
  step_dummy(all_nominal())

simple_ames_recipe
```
## 6.2 using recipes

<h style="color: blue; font-size:12px;">The second phase for using a recipe is to estimate any quantities required by the steps using the prep()</h>

```{r}
#using the recipe, prep the train data

(simple_ames_prepped = simple_ames_recipe %>% prep(training = train))
```

## 6.3 encoding qualitative data in a numeric format
```{r}
train %>% count(neighborhood, name = 'count') %>% plot_ly(x = ~count, y = ~neighborhood) %>% add_bars()
```
<h style="color: blue; font-size:14px;">For a specific factor, let's collapse (into an 'other' level) all levels that have fewer counts than 1% of total level counts'</h>

<h style="color: blue; font-size:14px;">in this example, let's collapse the specific neighborhoods that have a total count less than 1% of the total neighborhood counts'</h>

```{r}
simple_ames_recipe <- 
  recipe(sale_price ~ neighborhood + gr_liv_area + year_built + bldg_type,
         data = train) %>%
  step_log(gr_liv_area, base = 10) %>% 
  step_other(neighborhood, threshold = 0.01) %>% #setting threshold of the 'other category'
  step_dummy(all_nominal())

simple_ames_recipe
```



# Chapter 7: Fitting models with parsnip
# Chapter 8: A model workflow
# Chapter 9: Judging model effectiveness
# TOOLS: FOR CREATING EFFECTIVE MODELS
# Chapter 10: Resampling for evaluating performance
# Chapter 11: Comparing models with resampling
# Chapter 12: Model tuning and the dangers of overfitting
# Chapter 13: Grid search
# Chapter 14: Iterative search
# Chapter 15: Explaining models and predictions
