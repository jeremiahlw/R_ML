---
title: "Housing Prices"
subtitle: "Predicting Housing Prices using RF, ENET, XGBST"
author: 'Jeremiah Wang'
date: "December 2020"
output:
  html_document:
    theme: simplex
    highlight: pygments
    code_folding: hide
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
    toc_depth: 2
    df_print: paged
---

```{r}
#https://datatables.net/reference/option/
options(DT.options = list(scrollX = TRUE, pagin=TRUE, fixedHeader = TRUE, searchHighlight = TRUE))
```

```{r include=FALSE}
library(DataExplorer);library(data.table);library(dlookr);
library(extrafont);library(formattable);library(funModeling);library(GGally);
library(here);library(janitor);library(lubridate);library(naniar);
library(PerformanceAnalytics);library(plotly);library(RColorBrewer);
library(readxl);library(skimr);library(tidyverse);library(scales);
library(visdat)

library(tidymodels)
```

# Intro

[Check this Kaggle](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/data)

# Objective

Predict Housing Prices using 3 supervised ml algos:

1. elastic net
2. random forest
3. xgboost

# Get & Split Data
```{r}
a = read_csv('train.csv') %>% 
  clean_names() %>% 
  mutate(across(where(is.character), factor)) %>% 
  select(sort(tidyselect::peek_vars())) %>% 
  select(
    where(is.Date),
    where(is.factor),
    where(is_character),
    where(is_numeric)
  )

test = read_csv('test.csv') %>% 
  clean_names() %>% 
  mutate(across(where(is.character), factor)) %>% 
  select(sort(tidyselect::peek_vars())) %>% 
  select(
    where(is.Date),
    where(is.factor),
    where(is_character),
    where(is_numeric)
  )

a %>% sample_n(10)
```

# 5 min EDA

### viz missing
```{r fig.width=12, fig.height=9}
total.cols = a %>% colnames() %>% length() 

a %>% visdat::vis_dat()
```

### check missing, na, inf
```{r}
a %>% select(where(is.factor)) %>% funModeling::status() %>% arrange(-q_na)
a %>% select(where(is.numeric)) %>% funModeling::status() %>% arrange(-q_na)
```

Observations

1. 4 factor vars have 80%+ missing values
    + since this is so many, consider removing entirely rather than imputing
2. For other factor vars with minimal missing nas, consider either:
    + replacing missing values with dedicated 'unknown' level OR
    + replacing missing values by imputing
3. Consider imputing num vars with nas
    + lot_frontage: rf or kmeans imputation with all other vars
    + garage_yr_blt: rf imputation with other date vars (e.g. year_XXX, yr_sold)
    + mas_vnr_area: rf or kmeans imputation with all other vars


# Wrangling / Cleaning
```{r eval=FALSE}
a$pool_qc = NULL
a$misc_feature = NULL
a$alley = NULL
a$fence = NULL
```

# EDA: nom vars

### names
```{r}
(nom.vars = a %>% select(where(is.factor)) %>% colnames %>% as.character)
```

### check missing
```{r}
a %>% select(nom.vars) %>% miss_var_summary
```

### sample
```{r}
a %>% select(nom.vars) %>% sample_n(5)
```

### skim
```{r}
a %>% select(nom.vars) %>% skim_without_charts()
```

### viz: level counts distribution
```{r}
a %>% select(nom.vars) %>% map(n_unique) %>% as.data.frame.list %>% pivot_longer(everything()) %>% mutate(name = fct_reorder(name, value)) %>% plot_ly(x = ~value, y = ~name, color = ~name) %>% hide_legend() %>% layout(title = 'factor vars level counts', xaxis = list(title = 'count'), yaxis = list(title = ''))

a %>% select(nom.vars) %>% funModeling::freq()
```

### viz: mosaic plots: ggpairs
```{r fig.width=12, fig.height=9}
#documentation: https://mran.microsoft.com/snapshot/2016-01-12/web/packages/GGally/vignettes/ggpairs.html

a %>% select(nom.vars) %>% 
  GGally::ggpairs(
    columns = c(nom.vars[13], nom.vars[32]),
    mapping = aes(color = eval(as.name(nom.vars[13])))
)

```

Description of Masonry veneer type

+ BrkCmn	Brick Common
+ BrkFace	Brick Face
+ CBlock	Cinder Block
+ None	None
+ Stone	Stone

Description of Exterior Quality Levels

+ Ex	Excellent
+ Gd	Good
+ TA	Average/Typical
+ Fa	Fair
+ Po	Poor

<h style="color: blue; font-size:16px;">Not too surprisingly, Many of the Brick type Masonry veneer type had Exterior Qualities of Good and Excellent</h>

# EDA: num vars

### names
```{r}
(num.vars = a %>% select(where(is.numeric)) %>% colnames %>% as.character)
```

### check missing
```{r}
a %>% select(num.vars) %>% miss_var_summary
```

### sample
```{r}
a %>% select(num.vars) %>% sample_n(5)
```

### skim
```{r}
a %>% select(num.vars) %>% skim_without_charts()
```

### viz: distribution - hist
```{r}
a %>% select(num.vars) %>% DataExplorer::plot_histogram(
  scale_x = 'log10',
  #geom_histogram_args = list(bins = 50L),
  ncol = 2, nrow = 2
)
```

### viz: distribution - density
```{r}
a %>% select(num.vars) %>% DataExplorer::plot_density(
  scale_x = 'log10',
  #geom_histogram_args = list(bins = 50L),
  ncol = 2, nrow = 2
)
```

### viz: outliers
```{r}
a %>% select(num.vars) %>% dlookr::plot_outlier()
```

### viz: normality
```{r}
a %>% select(num.vars) %>% dlookr::plot_normality()
```
### viz: normality: outcome var only
```{r}
a %>% select(sale_price) %>% dlookr::plot_normality()
```
<h style="color: blue; font-size:16px;">consider log transforming</h>

### viz: correlations

```{r fig.width= 12, fig.height=9}
a %>% select(num.vars) %>% visdat::vis_cor()

a %>% select(num.vars) %>% dlookr::plot_correlate()

a %>% select(num.vars) %>% GGally::ggcorr(low = 'red', high = 'darkgreen', label = TRUE)
```

Observations:

1. Not surprisingly there are positive cor among certain vars (list below not comprehensive):
    + gr_liv_area & tot_rms_abv_grd
    + gr_liv_area & sale_price
    + bedroom_abv_gr & tot_rms_abv_grd
    + yr_blt & garage_yr_blt
2. Consider doing PCA on correlated vars

# EDA: nom/num vars

```{r}
target.var = 'sale_price'
```
### viz: cor w/target - pearson
```{r}
a %>% select(num.vars) %>% funModeling::correlation_table(target = target.var)
```

<h style="color: blue; font-size:16px;">quick and dirty pseudo feature importance</h>


### feature engineering a binary var for viz below
```{r}
a %>% count(overall_qual) %>% arrange(-overall_qual)

a = a %>% mutate(
  overall_qual.gte.8.flg = if_else(
    overall_qual >= 8, 'yes', 'no'
  )
)

```


### viz: cross plot - input/target distribution
```{r eval=FALSE}
### useful when outcome var is a binary var
a %>% funModeling::cross_plot(
  input = nom.vars,
  target = "overall_qual.gte.8.flg"
  )
```

### quick analysis: binary target
```{r}
#This function is used to analyze data when we need to reduce variable cardinality in predictive modeling.
#works in conjunction with 'cross_plot';
a %>% funModeling::categ_analysis(
    input = nom.vars[13],
    target = 'overall_qual.gte.8.flg'
  )
```

<h style="color: blue; font-size:16px;">Of the subset of homes whose exterior quality was excellent, 88.5% had an overall quality rating of 8 or better </h>

# Preprocess Data

### 1) split data
```{r include=FALSE}
train = a %>% slice_sample(prop = 0.50)
test = test %>% slice_sample(prop = 0.50)
vfold = vfold_cv(train, v = 5)
```

```{r include=FALSE}
train %>% select(where(is.factor)) %>% miss_var_summary
train %>% select(where(is.factor)) %>% sample_n(5)
```

```{r include=FALSE}
train %>% select(where(is.numeric)) %>% miss_var_summary
train %>% select(where(is.numeric)) %>% sample_n(5)
```

### 2) create recipe
```{r}
#order reference: https://recipes.tidymodels.org/articles/Ordering.html

library(tidymodels)

rec.en = train %>% recipe(sale_price ~ . ) %>% 
  step_mutate(sale_price = log10(sale_price), skip = TRUE) %>% 
  step_rm(pool_qc, misc_feature, alley, fence) %>% #excessive nas
  step_bagimpute(
    lot_frontage, garage_yr_blt, mas_vnr_area
    ) %>% 
  step_unknown(all_nominal()) %>% #assign 'unknown' level to vars with nas
  step_corr(all_numeric(),-all_outcomes()) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>%  #There are new levels in a factor
  step_nzv(all_predictors(),-all_outcomes()) %>%
  step_zv(all_predictors(),-all_outcomes()) %>% 
  step_normalize(all_numeric(),-all_outcomes())

rec.en %>% tidy

#----------------------------

rec.rf = train %>% recipe(sale_price ~ . ) %>% 
  step_mutate(sale_price = log10(sale_price), skip = TRUE) %>% 
  step_rm(pool_qc, misc_feature, alley, fence) %>% #excessive nas
  step_bagimpute(all_numeric(),-all_outcomes()) %>% 
  step_knnimpute(all_nominal(),-all_outcomes()) %>%
  step_corr(all_numeric(),-all_outcomes()) %>%
  #step_dummy(all_nominal(), one_hot = TRUE) %>%  #There are new levels in a factor
  step_nzv(all_predictors(),-all_outcomes()) %>%
  step_zv(all_predictors(),-all_outcomes()) %>% 
  step_normalize(all_numeric(),-all_outcomes())

rec.rf %>% tidy


```

### 3) preprocess
```{r include=FALSE}
#rec %>% prep
rec.en %>% prep %>% bake(new_data = NULL) %>% head
rec.rf %>% prep %>% bake(new_data = NULL) %>% head
```

### check for missing data in processed data
```{r include=FALSE}
rec.en %>% prep %>% bake(new_data = NULL) %>% status %>% arrange(-p_na)
rec.rf %>% prep %>% bake(new_data = NULL) %>% status %>% arrange(-p_na)
```


### 4) create model spec
```{r}
mdl.en = parsnip::linear_reg(
  penalty = tune(),
  mixture = tune() #lasso/ridge mix
) %>% 
  set_mode('regression') %>% 
  set_engine('glmnet')

mdl.rf = parsnip::rand_forest(
  trees = 150,
  min_n = tune(), #min number of observations at terminal node
  mtry = tune() #number of vars to randomly subset at each node
) %>% 
  set_mode('regression') %>% 
  set_engine('ranger', importance = 'impurity_corrected')
```

### 5) create workflow spec
```{r}
wf.en = workflow() %>% 
  add_recipe(rec.en) %>% 
  add_model(mdl.en)
#----------------------------
wf.rf = workflow() %>% 
  add_recipe(rec.rf) %>% 
  add_model(mdl.rf)
```

### 6) execute workflow on vfold using **auto** hp tuning
```{r}
doParallel::registerDoParallel() #use parallel processing
set.seed(345)

tg.en = tune_grid(
  object = wf.en,
  resamples = vfold, 
  grid = 5 #Create a tuning grid AUTOMATICALLY
)

tg.en %>% collect_metrics()

#----------------------------
doParallel::registerDoParallel() #use parallel processing
set.seed(345)

tg.rf = tune_grid(
  object = wf.rf,
  resamples = vfold, 
  grid = 5 #Create a tuning grid AUTOMATICALLY
)

tg.rf %>% collect_metrics()
```

### viz results
```{r warning=FALSE}
ggplotly(
tg.en %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, penalty, mixture) %>%
  pivot_longer(penalty:mixture,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE, size = 3) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(title = 'Elastic Net RMSE by Hyperparameter', x = NULL, y = '')
)

tg.en %>% show_best('rmse')

#----------------------------

ggplotly(
tg.rf %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE, size = 3) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(title = 'Random Forest RMSE by Hyperparameter', x = NULL, y = '')
)

tg.rf %>% show_best('rmse')
```

### 7) choose best hps
```{r}
(best.hps.en = select_best(tg.en, 'rmse'))
#----------------------------
(best.hps.rf = select_best(tg.rf, 'rmse'))
```

### 8) finalize workflow, fit & execute model, evaluate metrics

```{r}
mdl.en.fit = wf.en %>% 
  #1) finalize wf (recipe, model w/previously unknown hps) using best hps
  finalize_workflow(best.hps.en) %>%
  #2) fit on entire train
  fit(train)

preds.en = mdl.en.fit %>% predict(test)

(10 ^ preds.en) %>% plot_ly(x = ~.pred)
(10 ^ preds.en) %>% plot_ly(x = ~.pred)  %>% add_boxplot() %>% layout(title = 'Distribution: Elastic Net Sale Price Predictions')

#----------------------------

mdl.rf.fit = wf.rf %>% 
  #1) finalize wf (recipe, model w/previously unknown hps) using best hps
  finalize_workflow(best.hps.rf) %>%
  #2) fit on entire train
  fit(train)

preds.rf = mdl.rf.fit %>% predict(test)

(10 ^ preds.rf) %>% plot_ly(x = ~.pred)
(10 ^ preds.rf) %>% plot_ly(x = ~.pred)  %>% add_boxplot() %>% %>% layout(title = 'Distribution: Random Forest Sale Price Predictions')

```

#gomi

1. 
2. compare bag impute vs step unknown
3. does step pca help/hurt
4 learn how to use vip package