---
title: "Supervised ML | Type: Numeric Response"
author: "Jeremiah Wang"
output:
    html_document:
      theme: paper
      highlight: default
      code_folding: hide
      df_print: paged
      toc: true
      toc_depth: 2
      toc_float:
        collapsed: false
        smooth_scroll: false
---
  
```{r}
# global options
#knitr::opts_chunk$set(echo=FALSE)
options(DT.options = list(scrollX = TRUE, paging=TRUE, fixedHeader=TRUE, searchHighlight = TRUE))

```

```{r include = FALSE}
# <style>
# .main-container {
#     width: 100%;
#     max-width: unset;
# }
# </style>
```

```{css echo=FALSE}
# special function defined to call out assumptions/notes
.note {
  color: red;
  font-weight: 700
}
```

```{r results='hide', message=FALSE, warning=FALSE}
#load packages
#default
library(DataExplorer);library(data.table);library(dlookr)
library(extrafont);library(formattable);library(GGally);library(here)
library(janitor);library(lubridate);library(naniar)
library(patchwork);library(PerformanceAnalytics)
library(plotly);library(RColorBrewer);library(readxl)
library(skimr);library(tidyverse);library(scales)
#rmarkdown
library(kableExtra)
#ml
library(caret);library(tidymodels);library(h2o)
```

# Get the Data

In one piped statement:
  
1. read in data
2. convert char to factor vars
3. rename all colnames lowercase
4. order cols by name: alphabetically
5. order cols by datatype: nominal, then numeric

```{r, message=FALSE}
a = read_csv('train.csv') %>%
  mutate(across(where(is.character),as.factor)) %>%
  clean_names(.) %>% select(sort(tidyselect::peek_vars())) %>%
  select(where(is.factor), where(is.numeric))
```

### split data using recipes package
```{r }
split = a %>% initial_split(strata = sale_price)
train = rsample::training(split)
test = rsample::testing(split)
```

# EDA: nom vars

### var reference
```{r}

```

### check head rows
```{r}
train %>% select(where(is.factor)) %>% head %>% DT::datatable()
```

### glimpse structure
```{r}
train %>% select(where(is.factor)) %>% glimpse
```

### check for missing values
```{r}
(miss_var_summary(train %>% select(where(is.factor))) %>% DT::datatable(options = list(scrollY = '500px')))
```

### notes about nom vars with missing data

1. PoolQC: Pool quality
2. MiscFeature: Miscellaneous feature not covered in other categories (e.g. elevator, 2nd garage, tennis court)
3. Alley: Type of alley access to property
4. Fence: Fence quality
5. FireplaceQu: Fireplace quality

### distribution: counts of unique levels
```{r}
sapply(train %>% select(where(is.factor)), n_unique) %>% as.data.frame() %>% arrange(desc(.)) %>% DT::datatable()
```

### reference: names of unique levels
```{r, eval=TRUE}
sapply(train %>% select(where(is.factor)), unique)
```

### distribution: viz
```{r fig.width=16, fig.height=12}
DataExplorer::plot_bar(train %>% select(where(is.factor)))
```

# EDA: num vars

### check head rows
```{r}
train %>% select(where(is.numeric)) %>% head %>% DT::datatable()
```

### glimpse structure
```{r}
train %>% select(where(is.numeric)) %>% glimpse
```

### check for missing values
```{r  rows.print = 10}
(miss_var_summary(train %>% select(where(is.numeric))) %>% DT::datatable(options = list(scrollY = '500px')))

```

### notes about num vars with missing data

1. LotFrontage: Linear feet of street connected to property
2. GarageYrBlt: Year garage was built
3. MasVnrArea: Masonry veneer area in square feet

### distribution: viz
```{r fig.width=12, fig.height=9}
DataExplorer::plot_boxplot(train %>% select(where(is.numeric)), by = 'sale_price')
```

### distribution: viz
```{r fig.width=12, fig.height=9}
DataExplorer::plot_histogram(train %>% select(where(is.numeric)))
```

### distribution: viz
```{r fig.width=12, fig.height=9}
DataExplorer::plot_density(train %>% select(where(is.numeric)))
```

### pairwise correlations: viz
```{r fig.width=16, fig.height=12}
GGally::ggcorr(train %>% select(where(is.numeric)), low = '#990000', mid = '#E0E0E0', high = '#009900', label = TRUE)
```


# EDA: other
### check distribution of response var in more detail: right skewed
```{r}
train %>% ggplot(aes(sale_price)) + geom_freqpoly()
train %>% ggplot(aes(sale_price)) + geom_histogram()
summary(train$sale_price)
```



### check for normality in distribution of dependent var
```{r}
qqnorm(train$sale_price)
qqline(train$sale_price, lwd = 2, col = 'darkred')
```

### log10 transformation helps restore normality
```{r}
qqnorm(log10(train$sale_price))
qqline(log10(train$sale_price), lwd = 2, col = 'darkred')
```

# Preprocessing

Reference: (package recipes)[https://recipes.tidymodels.org/reference/index.html]

### check ranges of num vars you want to bucket into nom vars
```{r}
(range(train$yr_sold)) #no need to cut, since there are so few levels 
(range(train$year_built))
(range(train$year_remod_add))
(range(train$garage_yr_blt, na.rm = TRUE))
```



### create preprocessing recipe
```{r}
(recipe = train %>% recipe(sale_price ~ . ) %>%
#need impute missing data in this var BEFORE binning/bucketing
   step_knnimpute(garage_yr_blt) %>%  #knnimpute uses gower distances; i.e. used on both nom,num vars
#cut year vars into 'lustrums', periods of 5 years each, and then convert to factor 
  step_mutate(yr_sold = as.factor(yr_sold)) %>% #no need to cut, since there are so few levels; simply convert 
  step_cut(year_built, breaks = seq(1870,2010,5), include_outside_range = TRUE) %>%       #bucket 
  step_cut(year_remod_add, breaks = seq(1950,2010,5), include_outside_range = TRUE) %>%   #bucket 
  step_cut(garage_yr_blt, breaks = seq(1900,2010,5), include_outside_range = TRUE) %>%    #bucket 
#convert num vars that s/b nom vars
  step_mutate(ms_sub_class = as.factor(ms_sub_class)) %>% 
  step_mutate(mo_sold = factor(mo_sold)) %>% 
#missing data imputation
  step_modeimpute(all_nominal(),-all_outcomes())%>% 
  step_unknown(all_nominal(),-all_outcomes()) %>% 
  step_medianimpute(all_numeric(),-all_outcomes()) %>% 
#handling potential multicollinearity via filtering
  step_corr(all_numeric(),-all_outcomes()) %>% 
  step_nzv(all_numeric(),-all_outcomes()) %>% 
  step_zv(all_numeric(),-all_outcomes()) %>% 
#Dummy Variables Creation
  #step_dummy(all_nominal(),-all_outcomes(),one_hot = TRUE)
#center and scale numeric data
  step_normalize(all_numeric(),-all_outcomes())
)
```
### examine how recipe will interact with train ds
```{r}
(recipe %>% prep())
```

## preprocess train ds & test ds
```{r}
#'Using the recipe, prepare & juice the train ds'
juiced.train = recipe %>% prep(retain=TRUE ) %>% juice
#'Using the recipe, prepare & bake the test ds'
baked.test = recipe %>% prep(retain=TRUE ) %>% bake(test)

juiced.train %>% head() %>% DT::datatable()
baked.test %>% head %>% DT::datatable()
```

### sanity check for missing data
```{r}
miss_var_summary(juiced.train)
miss_var_summary(baked.test)
```

### sanity check: nom vars - distribution: viz
```{r fig.width = 12, fig.height = 9}
juiced.train %>% select(where(is.factor)) %>% DataExplorer::plot_bar()
```

### sanity check: num vars - distribution: viz
```{r fig.width = 12, fig.height = 9}
juiced.train %>% select(where(is.numeric)) %>% DataExplorer::plot_histogram()
```

### distribution: viz
```{r fig.width=12, fig.height=9}
juiced.train %>% select(where(is.numeric)) %>% DataExplorer::plot_boxplot(by = 'sale_price')
```

### sanity check: num vars - pairwise correlations: viz
```{r fig.width= 20, fig.height= 20, cache= TRUE}
GGally::ggcorr(juiced.train %>% select(where(is.numeric)), low = '#990000', mid = '#E0E0E0', high = '#009900', label = TRUE)


```

# Simple Modeling

## Random Forest (ranger)
```{r}
#!!<NOTE> to analyze var importance, you need an importance arg
# The 'impurity_corrected' importance measure is unbiased in terms of the number of categories and category frequencies and is almost as fast as the standard impurity importance. 
model.rf = ranger::ranger(log10(sale_price) ~ . , data = juiced.train, num.trees = 300, importance = 'impurity_corrected')
model.rf
```

### viz var importance
```{r fig.width= 14, fig.height=10}
model.rf.var.imp = model.rf$variable.importance %>% as.matrix() %>% as.data.frame.matrix() %>% rownames_to_column() %>% rename(var = rowname, imp = V1) %>% arrange(-imp)

model.rf.var.imp %>% ggplot(aes(fct_reorder(var, imp), imp)) + geom_col() + coord_flip()
```

### make predictions
```{r }
model.rf.preds = as.vector(predict(model.rf, baked.test, seed = 88)$predictions)
```

### assess performance (rmse): average error of house price was $31k
```{r }
#y, pred
model.rf.df = tibble(y = baked.test$sale_price, preds = 10 ^ model.rf.preds) #undoing log transformation

#viz
model.rf.df %>% ggplot(aes(y, preds)) + geom_point() + geom_smooth(method='lm')

#what is rmse of model? ~31k 
model.rf.df %>% yardstick::rmse(truth = y, estimate = preds)
```

### compare relative performance
```{r}
#what does rmse look like rel. to sd of response var?
#what was sd of sales price? ~80k
sd(test$sale_price)
```

# Simple Modeling filtered to Top N VAR IMP

### find top N most important vars
```{r}
N = 15
(model.rf.var.imp.topN = model.rf.var.imp %>% head(N))
```

### Modeling and Assessment
```{r}
#filter data to top `r  N ` important vars
juiced.train.filt.imp.vars = juiced.train %>% select(sale_price, model.rf.var.imp.topN$var)

#create rf model
model.rf2 = ranger::ranger(log10(sale_price) ~ . , data = juiced.train.filt.imp.vars, num.trees = 300, importance = 'impurity_corrected')

#check out model
model.rf2

#check out factor importance
model.rf2.var.imp = model.rf2$variable.importance %>% as.matrix() %>% as.data.frame.matrix() %>% rownames_to_column() %>% rename(var = rowname, imp = V1) %>% arrange(-imp)

model.rf2.var.imp %>% ggplot(aes(fct_reorder(var, imp), imp)) + geom_col() + coord_flip()

#create preds
model.rf2.preds = as.vector(predict(model.rf2, baked.test, seed = 88)$predictions)

#y, pred
model.rf2.df = tibble(y = baked.test$sale_price, preds = 10 ^ model.rf2.preds) #undoing log transformation

#viz
model.rf.df %>% ggplot(aes(y, preds)) + geom_point() + geom_smooth(method='lm')

#what is rmse of model? ~32k 
model.rf2.df %>% yardstick::rmse(truth = y, estimate = preds)

```
Commentary

1. When using just the top `r N  ` variables, overall performance doesn't change much
2. Simpler models are faster, and easier to understand, so it's worth filtering down to just these variables in all modeling

# AutoML

(h2o AutoML documentation)[https://docs.h2o.ai/h2o-tutorials/latest-stable/h2o-world-2017/automl/index.html]

start h2o
```{r eval=TRUE}
#preprocess entire dataset
h2o.df = recipe %>% prep(retain=TRUE) %>% bake(a)

#select only the vars that were considered important
h2o.df = h2o.df %>% select(sale_price, model.rf.var.imp.topN$var)
#check for missing vars
miss_var_summary(h2o.df)
# Initialise h2o cluster
h2o.init()
# Convert data to h2o frame
h2o.hf <- as.h2o(h2o.df)
# describe dataset
h2o.describe(h2o.hf)
# identify target and features
y <- 'sale_price'
x <- setdiff(colnames(h2o.df), y)
# split data into train & validation sets
sframe <- h2o.splitFrame(h2o.hf, seed = 88)
train <- sframe[[1]]
test <- sframe[[2]]

aml <- h2o.automl(y = y,
                  training_frame = train,
                  leaderboard_frame = test,
                  max_runtime_secs = 30,
                  seed = 88
                  #project_name = "XXX"
                  )

#view leaderboard
print(aml@leaderboard)

#make predictions
aml.preds = predict(aml@leader, test)
```


### automl leader model assessment
```{r}
print(h2o.performance(aml@leader, test)) #29643.19
```


## Notes / Ideas
1. glm net
2. ohe
3. 
4. add the data description in 'appendix section' 
5. comparison with automatic EDA package dlookr, DataExplorer
## 
1. 
2. 

## Learning
1. (paged tables)[https://bookdown.org/yihui/rmarkdown/html-document.html]
2. (scrollable tables)[https://rstudio.github.io/DT/]
3. https://www.r-bloggers.com/be-aware-of-bias-in-rf-variable-importance-metrics/
4. (in what order should your preprocess vars?)things?[https://towardsdatascience.com/a-comprehensive-machine-learning-workflow-with-multiple-modelling-using-caret-and-caretensemble-in-fcbf6d80b5f2]
5. (AutoML)[http://htmlpreview.github.io/?https://github.com/h2oai/h2o-tutorials/blob/master/h2o-world-2017/automl/R/automl_regression_powerplant_output.html]

```{r eval=FALSE}
# bedroom_abv_gr  
# bsmt_full_bath  
# bsmt_half_bath
# fireplaces
# full_bath
# garage_cars
# half_bath
# mo_sold *
# ms_sub_class *
# overall_cond 
# overall_qual
# tot_rms_abv_grd

```

