---
title: "Predicting Number of Adult Bookings & ADR Prices by Hotel Type"
subtitle: 'Machine Learning: Supervised - Random Forest (RF)'
author: 'Jeremiah Wang'
date: "November 2020"
output:
  html_document:
    theme: simplex
    highlight: pygments
    code_folding: hide
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
    toc_depth: 2
    df_print: paged
---

```{r include=FALSE}
#https://datatables.net/reference/option/
options(DT.options = list(scrollX = TRUE, pagin=TRUE, fixedHeader = TRUE, searchHighlight = TRUE))

library(DataExplorer);library(data.table);library(dlookr);
library(extrafont);library(formattable);library(GGally);library(here);
library(janitor);library(lubridate);library(naniar);
library(PerformanceAnalytics);
library(plotly);library(RColorBrewer);library(readxl);
library(skimr);library(tidyverse);library(scales)

library(tidymodels)
```

# Introduction

[Check out this Kaggle](https://www.kaggle.com/jessemostipak/hotel-booking-demand)

# Business Goals

1. Predict the number of adults for the resort and city hotels for the next 4 weeks
    + Note: one booking can be for multiple adults, so aggregate accordingly
2. Predict average daily rate (adr) prices based on other features

# Ideas

1. fb's prophet package
2. random forest, elastic net

# Get Data

```{r message=FALSE}
a = read_csv('hotel_bookings.csv') %>%
  clean_names() %>% 
  mutate(across(where(is.character), factor)) %>% 
  select(sort(tidyselect::peek_vars())) %>% 
  select(
    where(is.Date),
    where(is.factor),
    where(is.numeric)
  ) %>% filter(is_canceled == 0) #filter to non-canceled bookings

a$is_canceled = NULL
a$reservation_status_date = NULL
```

# Resources

1. adr
    + https://www.xotels.com/en/glossary/adr-average-daily-rate/
2. transient
    + https://www.xotels.com/en/glossary/transient/
3. group rate
    + https://www.xotels.com/en/glossary/group-rate/
4. Guide to Hotel Management
    + https://www.4hoteliers.com/features/article/9886
5. distribution channels
    + https://www.xotels.com/en/glossary/channels/

## 5 min EDA

```{r}
a %>% head
```

<h style="color: red; font-size:18px;">Not sure exactly what 'reservation_status_date' refers to; for a date field, will create an 'arrival.date' col to use in its place</h>

```{r}
#I don't really understand this var; will create an 'arrival.date' var for a date field
a$reservation_status_date = NULL

skimr::skim(a)
```

### clean/encode data
```{r}
# make arrival date var
a = a %>% mutate(
  arrival.date = make_date(
    year = arrival_date_year,
    month = match(arrival_date_month, month.name),
    day = arrival_date_day_of_month)
  ) %>% arrange(arrival.date)

### will use this for later on
a.anomaly = a

# these numeric vars s/b factor vars
a = a %>% mutate_at(vars(arrival_date_day_of_month, arrival_date_week_number, arrival_date_year, is_repeated_guest), factor)

# reordering df
a = a %>% select(sort(tidyselect::peek_vars())) %>% 
  select(
    where(is.Date),
    where(is.factor),
    where(is.numeric)
  )


```

<h style="color: blue; font-size:16px;">You'll have to repeat the steps above for the test data when preprocessing with recipes and step_XXX'</h>

# EDA: time series

### range
```{r}
paste(
  'The date range of this dataset is from',
  a %>% pull(arrival.date) %>% range %>% .[1],
  'to',
  a %>% pull(arrival.date) %>% range %>% .[2],
  ', just over 3 years of data.'
)
```

### time series graph
```{r message=FALSE, warning=FALSE, out.width= 1000}
a %>% group_by(arrival.date) %>% 
  summarise(total.individuals = sum(adults, children, babies)) %>% 
  arrange(arrival.date) %>%
  plot_ly(
    x = ~arrival.date,
    y = ~total.individuals
  ) %>% layout(
    title = 'total.individuals by date',
    xaxis = list(title = ''),
    yaxis = list(title = '')
    )
```

### time series graph -- grouped (hotel)
```{r message=FALSE, warning=FALSE, out.width= 1000}
a %>% group_by(arrival.date, hotel) %>% 
  summarise(total.individuals = sum(adults, children, babies)) %>% 
  arrange(arrival.date) %>%
  plot_ly(
    x = ~arrival.date,
    y = ~total.individuals,
    color = ~hotel,
    alphtrain = 0.7
  )  %>% layout(
    title = 'total.individuals by date/hotel',
    xaxis = list(title = ''),
    yaxis = list(title = '')
    ) 
```

### time series graph -- grouped (customer_type)
```{r message=FALSE, warning=FALSE, out.width= 1000}
a %>% group_by(arrival.date, customer_type) %>% 
  summarise(total.individuals = sum(adults, children, babies)) %>%  
  arrange(arrival.date) %>%
  plot_ly(
    x = ~arrival.date,
    y = ~total.individuals,
    color = ~customer_type,
    alphtrain = 0.7
  ) %>% layout(
    title = 'total.individuals by date/customer_type',
    xaxis = list(title = ''),
    yaxis = list(title = '')
    )
```

### time series graph -- grouped (deposit_type)
```{r message=FALSE, warning=FALSE, out.width= 1000}
a %>% group_by(arrival.date, deposit_type) %>% 
  summarise(total.individuals = sum(adults, children, babies)) %>%   
  arrange(arrival.date) %>%
  plot_ly(
    x = ~arrival.date,
    y = ~total.individuals,
    color = ~deposit_type,
    alphtrain = 0.7
  ) %>% layout(
    title = 'total.individuals by date/deposit_type',
    xaxis = list(title = ''),
    yaxis = list(title = '')
    )
```

### time series graph -- grouped (distribution_channel)
```{r message=FALSE, warning=FALSE, out.width= 1000}
a %>% group_by(arrival.date, distribution_channel) %>% 
  summarise(total.individuals = sum(adults, children, babies)) %>%  
  arrange(arrival.date) %>%
  plot_ly(
    x = ~arrival.date,
    y = ~total.individuals,
    color = ~distribution_channel,
    alphtrain = 0.7
  ) %>% layout(
    title = 'total.individuals by date/distribution_channel',
    xaxis = list(title = ''),
    yaxis = list(title = '')
    )
```

# EDA: nom vars

### sample data
```{r}
a %>% select(where(is.factor)) %>% slice_sample(n = 10)
```

### glimpse structure
```{r}
a %>% select(where(is.factor)) %>% glimpse
```

### check missing values
```{r}
a %>% select(where(is.factor)) %>% miss_var_summary
```

### distribution of level counts per factor
```{r warning=FALSE}
jpal = colorRampPalette(brewer.pal(8,'Dark2'))(15)

a %>% select(where(is.factor)) %>%
  map(n_unique) %>%
  as.tibble() %>%
  pivot_longer(everything()) %>%
  plot_ly(y = ~name, x = ~value, color = ~name, colors = jpal) %>%
  add_bars() %>%
  hide_legend() %>% 
  layout(
    title = 'distribution of level counts per factor',
    xaxis = list(title = ''),
    yaxis = list(title = '')
    )
```

### reference: names of unique levels
```{r}
a %>% select(where(is.factor)) %>%
  map(unique)

a = a %>% mutate(arrival_date_month = factor(arrival_date_month, levels = c('January','February','March','April','May','June','July','August','September','October','November','December')))
```

# EDA: num vars

### check missing values
```{r}
a %>% select(where(is.numeric)) %>% miss_var_summary
```

### sample data
```{r}
a %>% select(where(is.numeric)) %>% slice_sample(n = 10)
```

### glimpse structure
```{r}
a %>% select(where(is.numeric)) %>% glimpse
```


### viz: outliers
```{r}
a %>% select(where(is.numeric)) %>% dlookr::plot_outlier()
```

<h style="color: red; font-size:18px;">There are many upper outliers.  When building a prediction model, perhaps we should consider transforming them or removing them entirely.</h>

### check right tailness
```{r}
jquantiles = function(col){quantile(col, probs = c(0.90, 0.95, 0.99, 1))}

a %>% na.omit %>% select(where(is.numeric)) %>%
  map(.x = . , jquantiles) %>%
  as.data.frame.list() %>%
  rownames_to_column() %>%
  as.tibble()

```

### viz: normality
```{r}
a %>% select(where(is.numeric)) %>% dlookr::plot_normality()
```

### viz: distribution histogram (with outliers)
```{r}
#with outliers
a %>% select(where(is.numeric)) %>% DataExplorer::plot_histogram(nrow = 2, ncol = 1)
```

### viz: distribution histogram (without most extreme outliers)

```{r}
#no extreme right tailed outliers
a %>% select(where(is.numeric)) %>% filter(
  adr != 510,
  adults != 55,
  babies != 10,
  booking_changes != 21,
  children != 10,
  days_in_waiting_list != 391,
  lead_time != 709,
  previous_bookings_not_canceled != 72,
  previous_cancellations != 26,
  required_car_parking_spaces != 8,
  stays_in_week_nights != 50,
  stays_in_weekend_nights != 19
) %>% DataExplorer::plot_histogram(nrow = 2, ncol = 1)
```


### viz: distribution bivariate (without extreme outliers)
```{r}
#no outliers
a %>% select(hotel, where(is.numeric)) %>% filter(
  adr != 510,
  adults != 55,
  babies != 10,
  booking_changes != 21,
  children != 10,
  days_in_waiting_list != 391,
  lead_time != 709,
  previous_bookings_not_canceled != 72,
  previous_cancellations != 26,
  required_car_parking_spaces != 8,
  stays_in_week_nights != 50,
  stays_in_weekend_nights != 19
) %>% DataExplorer::plot_boxplot(by = 'hotel', nrow = 3, ncol = 1)
```

# EDA: ADR further investigation

<h style="color: red; font-size:18px;">CAUTION: You should calculate ADR per adult since the ds is at the booking level not the individual level.  Also you should remove bookings with children</h>


```{r}
#check: check number of bookings by combination of adult and children counts
a %>% count(adults, children)

a %>% count(adults, children) %>% filter(adults == 0) %>% summarise(total.bookings.without.adults = sum(n))
a %>% count(adults, children) %>% filter(children == 0) %>% summarise(total.bookings.without.children = sum(n))
a %>% count(adults, children) %>% filter(children == 0, adults == 0) %>% summarise(total.bookings.without.adults.and.children = sum(n))

```
<h style="color: blue; font-size:16px;">There are very few bookings without adults, but it's good to filter out to get the most accurate results.'</h>

### percent of ds with only adult booking data
```{r}
paste(
  scales::percent(
    nrow(a %>% filter(adults > 0, children == 0)) / nrow(a)
    ),
  'of all observations have 1 or more adults and zero children.'
)

a.adult.adrs = a %>% filter(adults > 0, children == 0)
```

### distribution ADR per child
```{r}
a %>% filter(adults == 0, children >0) %>% mutate(adr.per.child = adr/children) %>% plot_ly(x = ~adr.per.child)  %>% add_boxplot() %>% layout('ADR distribution per child')
```

<h style="color: blue; font-size:16px;">Median ADR per child is $43.54</h>

### create adr.per.adult var
```{r warning=FALSE, out.width= 1000}
#creating adr per adult var
a.adult.adrs = a.adult.adrs %>% mutate(adr.per.adult = adr/adults)

a.adult.adrs %>%
  plot_ly(y = ~hotel, x = ~adr.per.adult, color = ~hotel, colors = jpal[1:2]) %>% 
  add_boxplot() %>% 
  layout(
    title = 'ADR per Adult by Hotel Type',
    xaxis = list(title = ''),
    yaxis = list(title = '')
  )

#https://stackoverflow.com/questions/57300053/split-a-plotly-boxplot-x-axis-by-group
a.adult.adrs %>%
  plot_ly(y = ~hotel, x = ~adr.per.adult, color = ~customer_type, colors = jpal, group = ~customer_type) %>% 
  add_boxplot() %>% 
  layout(
    boxmode = 'group', #SUPER IMPORTANT
    title = 'ADR per Adult by Hotel/customer_type'
    ) 

#https://stackoverflow.com/questions/57300053/split-a-plotly-boxplot-x-axis-by-group
a.adult.adrs %>%
  plot_ly(y = ~hotel, x = ~adr, color = ~market_segment, colors = jpal, group = ~market_segment) %>% 
  add_boxplot() %>% 
  layout(
    boxmode = 'group', #SUPER IMPORTANT
    title = 'ADR per Adult by Hotel/market_segment'
    ) 

#https://stackoverflow.com/questions/57300053/split-a-plotly-boxplot-x-axis-by-group
a.adult.adrs %>%
  plot_ly(x = ~hotel, y = ~adr, color = ~arrival_date_month, colors = jpal, group = ~arrival_date_month) %>% 
  add_boxplot() %>% 
  layout(
    boxmode = 'group', #SUPER IMPORTANT
    title = 'ADR per Adult by Hotel/arrival_date_month',
    hoverformat = '.0f'
    ) 

library(DescTools)


```

Observations

1. On average, the Resort Hotel (\$40) is cheaper than the City Hotel (\$54)
2. Not surprisingly, 'Direct' market segment customers pay more, while Group and Corporate customers pay less
3. Not surprisingly, ADR rates are higher for the Resort Hotel in summer months


### correlations: viz
```{r fig.width= 12, fig.height=9}
a %>% select(where(is.numeric)) %>% dlookr::plot_correlate()
a %>% select(where(is.numeric)) %>% GGally::ggcorr(palette = "RdBu", label = TRUE)
```

# EDA: viz: Paired XXXvariate
```{r fig.width=12, fig.height=9}
a %>% select(
  hotel,
  customer_type
) %>% GGally::ggpairs(
  mapping = aes(color = hotel)
  ) + scale_fill_manual(values = c('darkblue','darkorange'))
```

# Goal 1: Predict Number of *Adults* by Hotel Type for the next 4 weeks

<h style="color: blue; font-size:16px;">Aggregating at the weekly level to smooth out daily fluctuations</h>

### creating time series dataset

<h style="color: blue; font-size:16px;">creating a separate time series ds for anomaly detection and predictions b/c time series ds needs to be split chronologically via 'initial_time_split', not randomly via 'initial_split'

### create ts dataset, filter, and split
```{r}
#times series dataset
ts =  a %>% na.omit %>% filter(
  adults > 0,
  children == 0
) %>% mutate(
  arrival.date = lubridate::make_date(
      year = arrival_date_year,
      month = match(arrival_date_month, month.name),
      day = arrival_date_day_of_month
  )) %>% mutate(
    arrival.week = floor_date(arrival.date, 'week', week_start = 7)
  ) %>% group_by(arrival.week, hotel) %>% 
  summarise(total.adult.bookings = sum(adults), .groups = 'drop') %>% 
  arrange(arrival.week)
  
#---------------------------- city

ts.city = ts %>% filter(hotel == 'City Hotel')
ts.city$hotel = NULL
ts.city.split = initial_time_split(ts.city)
ts.city.a = training(ts.city.split)
ts.city.test = testing(ts.city.split)

#---------------------------- resort

ts.resort = ts %>% filter(hotel == 'Resort Hotel')
ts.resort$hotel = NULL
ts.resort.split = initial_time_split(ts.resort)
ts.resort.a = training(ts.resort.split)
ts.resort.test = testing(ts.resort.split)

```


## Anomaly Detection
```{r warning=FALSE, message=FALSE}
library(anomalize)
# time_decompose(data, target, method = c("stl", "twitter"), frequency = "auto", trend = "auto", ..., merge = FALSE, message = TRUE)
# anomalize(data, target, method = c("iqr", "gesd"), alpha = 0.05, max_anoms = 0.2, verbose = FALSE)
# The alpha parameter adjusts the width of the critical values. By default, alpha = 0.05.
# Lower values are more conservative while higher values are less prone to incorrectly classifying "normal" observations.
# max_anoms: The maximum percent of anomalies permitted to be identified.

# The STL method uses the stl() function from the stats package. STL works very well in circumstances where a long term trend is present (which applies in this case; see trend component in the prophet graphs below'). 
  
(ts.city.a %>% as.tibble() %>% 
  time_decompose(total.adult.bookings, method = 'stl', merge = TRUE) %>%
  anomalize(remainder, alpha = 0.15, method = 'gesd') %>% #increasing sensitivity to outliers
  time_recompose())

ggplotly(
  ts.city.a %>% as.tibble() %>% 
  time_decompose(total.adult.bookings, method = 'stl', merge = TRUE) %>%
  anomalize(remainder, alpha = 0.15, method = 'gesd') %>% #increasing sensitivity to outliers
  time_recompose() %>% 
    plot_anomalies(
      ncol = 2,
      alpha_dots = 0.5,
      alpha_circles = 0.5,
      size_circles = 2,
      time_recomposed = TRUE,
      alpha_ribbon = 0.05
      ) + scale_y_continuous(labels = comma) +
    labs(x = '', y = 'total.bookings', title = 'city hotel total.adult.bookings')
  )

#----------------------------

(ts.resort.a %>% as.tibble() %>% 
  time_decompose(total.adult.bookings, method = 'stl', merge = TRUE) %>%
  anomalize(remainder, alpha = 0.15, method = 'gesd') %>% #increasing sensitivity to outliers
  time_recompose())

ggplotly(
  ts.resort.a %>% as.tibble() %>% 
  time_decompose(total.adult.bookings, method = 'stl', merge = TRUE) %>%
  anomalize(remainder, alpha = 0.15, method = 'gesd') %>% #increasing sensitivity to outliers
  time_recompose() %>% 
    plot_anomalies(
      ncol = 2,
      alpha_dots = 0.5,
      alpha_circles = 0.5,
      size_circles = 2,
      time_recomposed = TRUE,
      alpha_ribbon = 0.05
      ) + scale_y_continuous(labels = comma) +
    labs(x = '', y = 'total.bookings', title = 'resort hotel total adults')
  )
  

```

## Predicting Next 4 Wks Total Adults by Hotel Type

### City Hotel
```{r}
library(prophet)
# https://dygraphs.com/options.html

#renaming cols to prophet's col conventions
prophet.city.df = ts.city.a %>% select(ds = arrival.week, y = total.adult.bookings)

#creating model
prophet.city.mdl = prophet.city.df %>% prophet(yearly.seasonality = TRUE)

#using model make future period df
prophet.city.future.df = prophet.city.mdl %>% make_future_dataframe(
  periods = 4, #next 4 wks
  freq = 'week',
  include_history = TRUE
  )

#make forecasts df
prophet.city.forecast.df = prophet.city.mdl %>% predict(prophet.city.future.df)

prophet.city.forecast.df %>% head %>% DT::datatable()

#plot forecast
prophet.city.mdl %>% dyplot.prophet(
  prophet.city.forecast.df,
  main = '<h style="color: black; font-size:18px;">City Hotel: Total Adults 4 Week Prediction</h>'
  ) %>%
  dygraphs::dyOptions(
    colors = c('darkgreen','blue'),
    pointSize = 2,
    )

#plot forecast components
prophet.city.mdl %>% prophet_plot_components(prophet.city.forecast.df)

```

### measure model performance - city
```{r}
(prophet.city.preds.truth = tibble(
  arrival.date = ts.city.test %>% pull(arrival.week) %>% head(4),
  truth =  ts.city.test %>% pull(total.adult.bookings) %>% head(4),
  estimate = prophet.city.forecast.df %>% pull(yhat) %>% tail(4)
))

bind_rows(
prophet.city.preds.truth %>% yardstick::rmse(truth = truth, estimate = estimate),
prophet.city.preds.truth %>% yardstick::mae(truth = truth, estimate = estimate)
)

```


### Resort Hotel
```{r}
library(prophet)
# https://dygraphs.com/options.html

#renaming cols to prophet's col conventions
prophet.resort.df = ts.resort.a %>% select(ds = arrival.week, y = total.adult.bookings)

#creating model
prophet.resort.mdl = prophet.resort.df %>% prophet(yearly.seasonality = TRUE)

#using model make future period df
prophet.resort.future.df = prophet.resort.mdl %>% make_future_dataframe(
  periods = 4, #next 4 wks
  freq = 'week',
  include_history = TRUE
  )

#make forecasts df
prophet.resort.forecast.df = prophet.resort.mdl %>% predict(prophet.resort.future.df)

prophet.resort.forecast.df %>% head %>% DT::datatable()

#plot forecast
prophet.resort.mdl %>% dyplot.prophet(
  prophet.resort.forecast.df,
  main = '<h style="color: black; font-size:18px;">Resort Hotel: Total Adults 4 Week Prediction</h>'
  ) %>%
  dygraphs::dyOptions(
    colors = c('darkorange','blue'),
    pointSize = 2,
    )

#plot forecast components
prophet.resort.mdl %>% prophet_plot_components(prophet.resort.forecast.df)

```

```{r}
(prophet.resort.preds.truth = tibble(
  arrival.date = ts.resort.test %>% pull(arrival.week) %>% head(4),
  truth =  ts.resort.test %>% pull(total.adult.bookings) %>% head(4),
  estimate = prophet.resort.forecast.df %>% pull(yhat) %>% tail(4)
))

bind_rows(
prophet.resort.preds.truth %>% yardstick::rmse(truth = truth, estimate = estimate),
prophet.resort.preds.truth %>% yardstick::mae(truth = truth, estimate = estimate)
)

```

# Goal 2: Predict ADR

## 1) Split Data
```{r}
set.seed(345)
split = initial_split(a %>% slice_sample(prop = 0.20), strata = hotel) ##!!<NOTE> limiting data for speed purposes
train = rsample::training(split)
test = rsample::testing(split)
vfolds = vfold_cv(train, v = 10)
```

### level count for factor vars
```{r}
train %>% select(where(is.factor)) %>%
  map_df(n_unique) %>%
  pivot_longer(
    everything(),
    names_to = 'factor',
    values_to = 'unique.levels.count'
    ) %>% arrange(-unique.levels.count)
```

## 2) Create recipe
```{r}
# Documentation: https://www.rdocumentation.org/packages/recipes/versions/0.1.14

#a recipe is used for preprocessing
recipe = train %>% recipe(adr ~ . ) %>%
  #remove vars with low or now correlation
  step_corr(all_numeric(),-all_outcomes()) %>% 
  #remove vars with low or no variance
  step_nzv(all_numeric(),-all_outcomes()) %>% 
  step_zv(all_numeric(),-all_outcomes()) %>%
  #----------------------------
  #reduce number of levels for factors with many, many levels
  step_other(agent, company, country) %>%  #default threshold of 0.05
  #----------------------------
  step_normalize(stays_in_weekend_nights, stays_in_week_nights) %>% #prob not necessary b/c both vars are already scaled the same, but doing so anyway to develop good habits
  step_pca(stays_in_weekend_nights, stays_in_week_nights, num_comp = 1) #will limit to PC1 only
  
tidy(recipe)
```

### check interaction of recipe with vars
```{r}
(recipe %>% prep())
```

## 3) Preprocess train & test
```{r}
#'Using the recipe, prepare & bake the train ds'
baked.train = recipe %>% prep() %>% bake(new_data = NULL) %>%
  select(sort(tidyselect::peek_vars()))

#'Using the recipe, prepare & bake the test ds'
baked.test = recipe %>% prep() %>% bake(new_data = test) %>%
  select(sort(tidyselect::peek_vars()))

baked.train %>% head() %>% DT::datatable()
baked.test %>% head %>% DT::datatable()

```
### check that 'step_other' worked correctly
```{r}
baked.train %>% select(agent, company, country) %>% map(levels)
baked.train %>% select(agent, company, country) %>% map(fct_count, sort = TRUE)
```

# Simple Modeling
```{r eval=TRUE}
doParallel::registerDoParallel() #use parallel processing

#!!<NOTE> to analyze var importance, you need an importance arg
# The 'impurity_corrected' importance measure is unbiased in terms of the number of categories and category frequencies and is almost as fast as the standard impurity importance. 
model.rf = ranger::ranger(adr ~ . , data = baked.train, num.trees = 300, importance = 'impurity_corrected')

model.rf

#viz var importance
model.rf.var.imp = model.rf$variable.importance %>% as.matrix() %>% as.data.frame.matrix() %>% rownames_to_column() %>% rename(var = rowname, imp = V1) %>% arrange(-imp)

model.rf.var.imp %>% ggplot(aes(fct_reorder(var, imp), imp)) + geom_col() + coord_flip()

#make predictions
model.rf.preds = as.vector(predict(model.rf, baked.test, seed = 345)$predictions)

#y, pred
model.rf.df = tibble(y = baked.test$adr, preds = model.rf.preds)

#viz
model.rf.df %>% ggplot(aes(y, preds)) + geom_point() + geom_smooth(method='lm')

#what is rmse and mae?
bind_rows(
  rmse = model.rf.df %>% yardstick::rmse(truth = y, estimate = preds),
  mae = model.rf.df %>% yardstick::mae(truth = y, estimate = preds)
  )
#compare relative performance
#what does mae look like rel. to sd of response var?
paste("test set's standard deviation is: ", round(sd(test$adr),1))
```

Observations

1. Due to seasonality, it's not surprising how date vars ranking highly in var importance
2. Reserved Room type is also self-explanatory
3. Ditto for market segment (e.g. Group rates are cheaper than Impromptu 'Transient' bookings)
4. Interestingly, type of hotel (City, Resort) is ~equal in var imp. to type of agent
    + looking at the agent levels above, perhaps having not agent (NULL) or agent 9 makes a decent impact on adr

# Intermediate Modeling

### reference of var abbreviations
```{r}
tibble(
  abbreviation = c('rec','mdl','wf','tg'),
  term = c('recipe','model','workflow','tune_grid'),
  description = c('executes necessary preprocessing steps on the features','self explanatory','combines the recipe and model','executes on vfolds using the workflow, and optional hyperparameter grid')
) %>% DT::datatable()
```

### 1) Create model *Specification* w/hps to be tuned
```{r}
#hyperparameters with a value of 'tune()' are those we will 'tune'
rf.mdl = parsnip::rand_forest(
  trees = 300,
  min_n = tune(), #min number of observations at terminal node
  mtry = tune() #number of vars to randomly subset at each node
) %>% 
  set_mode('regression') %>% 
  set_engine('ranger', importance = 'impurity_corrected')
```

### 2) Create workflow *Specification*
```{r}
#create workflow (pipeline) combining recipe (preprocessing) and model (w/hps)
rf.wf = workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(rf.mdl)
```

### 3) Execute model & workflow on vfold ds using **auto**-hps *Execution*
```{r}
doParallel::registerDoParallel() #use parallel processing
set.seed(345)

rf.tg = tune_grid(
  rf.wf,
  resamples = vfolds,
  grid = 5) #Create a tuning grid AUTOMATICALLY

rf.tg

rf.tg %>% collect_metrics()
```

### viz results
```{r warning=FALSE}

ggplotly(
rf.tg %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE, size = 3) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "RMSE")
)

rf.tg %>%
  collect_metrics() %>% 
  filter(.metric == 'rmse') %>% 
  select(mean, mtry, min_n, .config) %>% 
  arrange(mean)

```

<h style="color: blue; font-size:18px;">based on the results above, we can now get an idea of the ranges we should manually set for our hps</h>

### 4) Create a tuning grid manually
```{r}
rf.gr = grid_regular(
  min_n(range = c(25, 35)),
  mtry(range = c(15,25)),
  levels = 3
)

rf.gr
```


### 5) Execute model & workflow on vfold ds using **manual**-hps *Execution*

```{r}
doParallel::registerDoParallel() #use parallel processing
set.seed(345)

rf.tg = tune_grid(
  rf.wf,
  resamples = vfolds,
  grid = rf.gr) #Create a tuning grid MANUALLY

rf.tg

rf.tg %>% collect_metrics()
```

### viz results
```{r}
ggplotly(
rf.tg %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, min_n, mtry) %>%
  pivot_longer(min_n:mtry,
    values_to = "value",
    names_to = "parameter"
  ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(show.legend = FALSE, size = 3) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "RMSE")
)

rf.tg %>%
  collect_metrics() %>% 
  filter(.metric == 'rmse') %>% 
  select(mean, mtry, min_n, .config) %>% 
  arrange(mean)
  
```

###  6) choose best hps
```{r}
(rf.best.hps = select_best(rf.tg, 'rmse'))
```

### 7) finalize model
```{r}
rf.mdl.fin = finalize_model(
  x = rf.mdl, #the model
  parameters = rf.best.hps
)
```

### 9) Create final workflow, Execute on test, Evaluate results
```{r}
#create final workflow
rf.wf.fin = workflow() %>% 
  add_recipe(recipe) %>% 
  add_model(rf.mdl.fin)

#fit on ENTIRE train, execute on test, evaluate results
rf.final = rf.wf.fin %>% 
  last_fit(split)

#check results (standard metrics)
rf.final %>% collect_metrics()
```
### Evaluate additional metrics
```{r}
rf.final.results = rf.final %>% collect_predictions() %>% select(.pred, adr)

rf.metric.set = metric_set(rmse, rsq, mae)

rf.metric.set(rf.final.results, truth = adr, estimate = .pred)

```

### 10) Examine var imp (test)
```{r}
rf.final %>% 
  pluck(".workflow", 1) %>%   
  pull_workflow_fit() %>% 
  vip::vip(num_features = 10)

```

Observations:

1. Due to seasonality, it's not surprising how date vars ranking highly in var importance
2. Reserved Room type is also self-explanatory
3. Ditto for market segment (e.g. Group rates are cheaper than Impromptu 'Transient' bookings)
4. Interestingly, type of hotel (City, Resort) is ~equal in var imp. to type of agent














