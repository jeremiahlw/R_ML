---
title: "Wind Turbines"
title: "Predicting Power using XGBoost & Interpreting using SHAP"
author: 'Jeremiah Wang'
date: 'January 2020'
output:
  html_document:
    theme: simplex
    highlight: pygments
    code_folding: hide
    toc: yes
    toc_float:
      collapsed: no
      smooth_scroll: no
    toc_depth: 2
    df_print: paged
---
 
```{r include=FALSE}
#https://datatables.net/reference/option/
options(DT.options = list(scrollX = TRUE, pagin=TRUE, fixedHeader = TRUE, searchHighlight = TRUE))
options(scipen = 5, digits = 2)
library(beepr);library(DataExplorer);library(data.table);library(dlookr);
library(extrafont);library(formattable);library(funModeling);library(GGally);
library(here);library(janitor);library(lubridate);library(naniar);
library(plotly);library(RColorBrewer);
library(readxl);library(seplyr);library(skimr);library(tidyverse);library(scales);
library(visdat)

library(tidymodels);library(usemodels)

library(SHAPforxgboost)
```

# Objective

1. Predict the capacity of wind turbines in Canada based on turbine features using xgboost
2. Interpret model using SHAP values

# Get Data
```{r}
a = read_csv("https://raw.githubusercontent.com/rfordatascience/tidytuesday/master/data/2020/2020-10-27/wind-turbine.csv")
#abak = a
a = readRDS('a.rds') %>% 
  clean_names() %>% 
  mutate(across(where(is.character),factor)) %>% 
  select(sort(tidyselect::peek_vars())) %>% 
  select(
    where(is.Date),
    where(is.factor),
    where(is.numeric)
  ) %>% slice_sample(prop = 0.333) #for speed, ony using 1/3 of orig dataset

a %>% sample_n(5)

#remove unnecessary cols

a = a %>% select(
    -notes,
    -turbine_identifier,
    -turbine_number_in_project,
    -objectid,
    -total_project_capacity_mw,
    #removing the vars below b/c it would be unrealistic to use them for predictions
    #we likely wouldn't have such information with new data
    -model, 
    -latitude,
    -longitude,
    -project_name
)
a %>% sample_n(5)
```

### check zeroes, miss, inf
```{r}
a %>% vis_dat()
a %>% vis_miss()
a %>% status() %>% arrange(-p_na)

#Since there are so few rows with missin data, will simply remove missing rows, and not bother with imputation
a = a %>% na.omit()
```

### Express EDA
```{r out.width=800}
#normally I would do more thorough EDA, but the purpose of this notebook is to practice creating and interpreting XGBoost Models
library(inspectdf)
a %>% inspect_num()
a %>% inspect_cat() 
a %>% inspect_cat() %>% .$levels
a %>% inspect_cor()
a %>% inspect_types()

library(WVPlots)
#https://cran.r-project.org/web/packages/WVPlots/vignettes/WVPlots_examples.html
a %>% ScatterHist('rotor_diameter_m','turbine_rated_capacity_k_w', smoothmethod = 'lm','rotor diameter vs turbine output')
a %>% ScatterHist('hub_height_m','turbine_rated_capacity_k_w', smoothmethod = 'lm','hub height vs turbine output')

library(ggpointdensity)
ggplotly(a %>% ggplot(aes(rotor_diameter_m, turbine_rated_capacity_k_w)) + geom_pointdensity(size = 1.2) + scale_color_viridis_c() + ggtitle('Rotor Diameter vs Turbine Output'))
ggplotly(a %>% ggplot(aes(hub_height_m, turbine_rated_capacity_k_w)) + geom_pointdensity(size = 1.2) + scale_color_viridis_c() + ggtitle('Hub Height vs Turbine Output'))
```

### 1) split data
```{r}
split = a %>% initial_split(strata = province_territory)
train = split %>% training
test = split %>% testing
vfold = train %>% vfold_cv
```

### 2) spec rec, mdl, wf
```{r}
rec = train %>% recipe(turbine_rated_capacity_k_w ~ . ) %>% 
  step_zv(all_numeric(), - all_outcomes()) %>% 
  step_nzv(all_numeric(), - all_outcomes()) %>% 
  step_corr(all_numeric(), - all_outcomes()) %>%
  #----------------------------
  step_other(manufacturer, - all_outcomes()) %>% 
  step_normalize(all_numeric(), - all_outcomes()) %>% 
  step_dummy(all_nominal(), - all_outcomes(), one_hot = TRUE)

baked.train = rec %>% prep %>% bake(new_data = NULL)
baked.test = rec %>% prep %>% bake(new_data = test)

#---------------------------------------------------

mdl = parsnip::boost_tree(
  trees = tune(),
  min_n = tune(),
  tree_depth = tune(),
  learn_rate = tune(),
  loss_reduction = tune(),
  sample_size = tune()
  ) %>% 
  set_mode("regression") %>% 
  set_engine("xgboost") 

#---------------------------------------------------

wf = workflow() %>% 
  add_recipe(rec) %>% 
  add_model(mdl)
```

### 3) execute tunegrid, finalize wf
```{r}
set.seed(345)
doParallel::registerDoParallel()

tg = tune_grid(
  object = wf,
  resamples = vfold,
  grid = 10
)

#---------------------------------------------------

wf = wf %>% finalize_workflow(tg %>% select_best())

beep(5)
```

### 4) using wf, fit train, finalize mdl
```{r}
(mdl = wf %>% fit(train) %>% pull_workflow_fit())
```

### 5) using wf, fit train, evaluate test, return results
```{r}
kekka = wf %>% last_fit(split)
kekka %>% collect_metrics()
kekka %>% collect_predictions() %>%
  select(.pred, turbine_rated_capacity_k_w) %>% 
  metric_set(rmse, mae, mape, rsq)(estimate = .pred, truth = turbine_rated_capacity_k_w)

ggplotly(
kekka %>% collect_predictions() %>%
  select(.pred, turbine_rated_capacity_k_w) %>% 
  ggplot(aes(.pred, turbine_rated_capacity_k_w)) +
  geom_abline(slope = 1, linetype = 'dotted', color = 'tomato3') +
  geom_point(alpha = 0.7, size = 1.5, color = 'slateblue3') +
  labs(
    title = 'Turbine Capacity (KW) Predictions vs Actuals', x = 'Predicted', y = 'Actuals'
  )
)

```

#SHAPforxgboost

### 0) create xgboost model
```{r message=FALSE}
X_train = baked.train %>% select(-turbine_rated_capacity_k_w) %>% as.matrix
y = baked.train$turbine_rated_capacity_k_w

(best.hps = tg %>% select_best())

#https://parsnip.tidymodels.org/reference/boost_tree.html#see-also

library(xgboost)

mdl.xgb = xgboost(
  data = X_train,
  label = y,
  max.depth = best.hps$tree_depth,
  nrounds = best.hps$trees,
  eta = best.hps$learn_rate,
  min.child.weight = best.hps$min_n,
  gamma = best.hps$loss_reduction,
  subsample = best.hps$sample_size,
  #----------------------------
  nthread = parallel::detectCores() - 2,
  verbose = FALSE,
  objective = 'reg:squarederror'
)
```

### 1) *Local SHAP* **shap.values**
```{r}
shap.vals = shap.values(
  mdl.xgb, X_train
)

#----------------------------top 10 Features avg SHAP values
tibble(
features = shap.vals$mean_shap_score %>% sort(decreasing = TRUE) %>% head(10) %>% names,
SHAP = shap.vales$mean_shap_score %>% sort(decreasing = TRUE) %>% head(10)
) %>% mutate(features = fct_reorder(features, SHAP)) %>% 
  plot_ly(x = ~SHAP, y = ~features) %>% 
  layout(
    title = 'Average SHAP of Top 10 Important Features',
    xaxis = list(title = ''),
    yaxis = list(title = '')
    )

#----------------------------individual observation's SHAP values 
observation = 88

shap.vals$shap_score %>% dplyr::slice(observation)

shap.vals$shap_score %>% dplyr::slice(observation) %>%
  pivot_longer(everything()) %>% 
  rename('feature' = 'name', 'SHAP' = 'value') %>% 
  filter(SHAP > 1 | SHAP < -1) %>%
  mutate(
    feature = fct_reorder(feature, SHAP),
    SHAP = round(SHAP, 1)
    ) %>% 
  plot_ly(x = ~SHAP, y = ~feature, color = ~if_else(SHAP < 0, I('darkred'), I('darkgreen'))) %>% 
  layout(
    title = paste0('SHAP Values of Important Features for Observation ', observation),
    xaxis = list(title = paste0(
      'Cumulative Total SHAP Values: ',
      round(shap.vals$shap_score %>% dplyr::slice(observation) %>% rowSums(), 2)
      )),
    yaxis = list(title = ''))

```

